{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q-1-2 every node\n",
    "###### Decision tree with categorical and numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### STEPS FOLLOWED:\n",
    "1. Read csv file to panda dataframe\n",
    "2. Apply one hot encoding to categorical features\n",
    "3. Split data to 80-20 for training and validation using train_test_split()\n",
    "4. Build Decision Tree using recursive function build_Tree().\n",
    "```\n",
    "   For numerical features: a. sort them on feature values along with class label\n",
    "                           b. calculate information gain on average of feature values where class label differs\n",
    "                           c. select point giving highest gain as the point to split that numerical feature \n",
    "```\n",
    "5. Apply predict method to predict class label and use inbuilt functions to calculate confusion matrix, classification report and accuracy score.\n",
    "6. Calculate the same measures using inbuilt scikitlearn decision tree to compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import log2 as log\n",
    "import pandas as pd\n",
    "from sklearn import tree, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "eps = np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input_data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get class label into Y and drop it from it from df and assign to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df.left\n",
    "X = df.drop(['left'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### perform one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pd.concat([X,pd.get_dummies(X['sales'],prefix='sales')],axis=1)\n",
    "Z = pd.concat([Z,pd.get_dummies(Z['salary'],prefix='salary')],axis=1)\n",
    "Z = Z.drop(['sales','salary'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split data into training(80%) and testing(20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(Z, Y,test_size=0.2)\n",
    "df1 = pd.concat([X_train, Y_train],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate entropy of class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_entropy(df):\n",
    "    class_label = df.keys()[-1]\n",
    "    class_entropy = 0\n",
    "    values = df[class_label].unique()\n",
    "    for val in values:\n",
    "        q = float(df[class_label].value_counts()[val])/len(df[class_label])\n",
    "        class_entropy += -q*log(q)\n",
    "    return class_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate entropy of a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_entropy(df, feature):\n",
    "    class_label = df.keys()[-1]\n",
    "    target_variables = df[class_label].unique()\n",
    "    variables = df[feature].unique()\n",
    "    entropy = 0\n",
    "    for var in variables:\n",
    "        ent = 0\n",
    "        for t in target_variables:\n",
    "            n = len(df[feature][df[feature]==var][df[class_label]==t])\n",
    "            d = len(df[feature][df[feature]==var])\n",
    "            q = n/(d+eps)\n",
    "            ent += -q*log(q+eps)\n",
    "        q2 = float(d)/len(df)\n",
    "        entropy += -q2*ent\n",
    "    return abs(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate the split point for numerical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_num_feature(data,cl_label,feature):\n",
    "    \n",
    "    max_ig = 0\n",
    "    max_split = None\n",
    "    pair = pd.concat([data,cl_label],axis=1)\n",
    "    pair = pair.sort_values(by=feature).reset_index()\n",
    "    found = set()\n",
    "    for i in xrange(len(data)-1):\n",
    "        if pair['left'][i]!=pair['left'][i+1] and (float(pair[feature][i] + pair[feature][i+1])/2) not in found:\n",
    "            found.add(float(pair[feature][i] + pair[feature][i+1])/2)\n",
    "            ig = compute_IG(pair,float(pair[feature][i] + pair[feature][i+1])/2, feature)\n",
    "            if ig > max_ig:\n",
    "                max_ig = ig\n",
    "                max_split = float(pair[feature][i] + pair[feature][i+1])/2\n",
    "    return max_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate IG of all features and select feature with maximum gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_attributes=['number_project','last_evaluation', 'satisfaction_level','average_montly_hours','time_spend_company']\n",
    "split_values={}\n",
    "\n",
    "def feature_to_select(df):\n",
    "    num_attr = [i for i in df.columns if i in numerical_attributes]\n",
    "    for at in num_attr:\n",
    "        split = split_num_feature(df[at],df['left'],at)\n",
    "        split_values[at]=split\n",
    "\n",
    "    gain = []\n",
    "    for key in df.keys()[:-1]:\n",
    "        gain.append(class_entropy(df)-feature_entropy(df,key))\n",
    "    return df.keys()[:-1][np.argmax(gain)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtable(df,node,value):\n",
    "    return df[df[node]==value].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helper function to calculate gain of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IG(df,val,feature):\n",
    "    cl_entropy = class_entropy(df)\n",
    "    l = df[df[feature]<val].reset_index(drop=True)\n",
    "    r = df[df[feature]>=val].reset_index(drop=True)\n",
    "    l_entropy = class_entropy(l)\n",
    "    r_entropy = class_entropy(r)\n",
    "    return cl_entropy - ( (float(len(l))/(len(df)+eps)*l_entropy) + (float(len(r))/(len(df)+eps)*r_entropy) )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change dataframe from numerical to 0 and 1 according to split point"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def change_df(df,val,feature):\n",
    "    df.loc[df[feature]<val,feature]=0\n",
    "    df.loc[df[feature]>=val,feature]=1\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate the split point for all numerical features and get split values for all in a dictionary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "numerical_attributes=['number_project','last_evaluation', 'satisfaction_level','average_montly_hours','time_spend_company']\n",
    "split_values={}\n",
    "\n",
    "for at in numerical_attributes:\n",
    "    split = split_num_feature(df1[at],df1['left'],at)\n",
    "    split_values[at]=split\n",
    "print split_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess data to perform prediction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def preprocess(df,split_values):\n",
    "    for at,data in split_values.iteritems():\n",
    "        change_df(df,data,at)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preprocess(df1,split_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### node of decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,feature,positive=0,negative=0):\n",
    "        self.feature=feature\n",
    "        self.split_pos=0\n",
    "        self.positive=positive\n",
    "        self.negative=negative\n",
    "        self.left=None\n",
    "        self.right=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function that generates the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Tree(df):\n",
    "    if len(df.columns)==1:\n",
    "        return None\n",
    "    \n",
    "    split_node = feature_to_select(df)\n",
    "    root = Node(split_node)\n",
    "    if split_node in numerical_attributes:\n",
    "        split_point = split_values[root.feature]\n",
    "        root.split_pos = split_point\n",
    "        \n",
    "        root.positive=len(df[df['left']>=split_point]['left'])\n",
    "        root.negative=len(df[df['left']<split_point]['left'])\n",
    "        \n",
    "        subtable_left = df[df[split_node]<split_point].reset_index(drop=True)\n",
    "        subtable_right = df[df[split_node]>=split_point].reset_index(drop=True)\n",
    "    \n",
    "    else:\n",
    "        root.positive=len(df[df['left']==1]['left'])\n",
    "        root.negative=len(df[df['left']==0]['left'])\n",
    "        \n",
    "        subtable_left = subtable(df,split_node,0)\n",
    "        subtable_right = subtable(df,split_node,1)\n",
    "    \n",
    "    subtable_left = subtable_left.drop(split_node,axis=1)\n",
    "    subtable_right = subtable_right.drop(split_node,axis=1)\n",
    "    \n",
    "    clValue_left,counts_left = np.unique(subtable_left['left'],return_counts=True)\n",
    "    clValue_right,counts_right = np.unique(subtable_right['left'],return_counts=True)\n",
    "    \n",
    "    if len(counts_left)>1:\n",
    "        root.left=build_Tree(subtable_left)\n",
    "    \n",
    "    if len(counts_right)>1:\n",
    "        root.right=build_Tree(subtable_right)\n",
    "        \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=build_Tree(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_predict(df,root,prediction):\n",
    "    if root==None:\n",
    "        return None\n",
    "\n",
    "    if root.feature in numerical_attributes:\n",
    "        try:\n",
    "            if root.right==None and root.left==None:\n",
    "                prediction.append(1 if root.positive>root.negative else 0)\n",
    "                return\n",
    "\n",
    "            if root.right==None and df[root.feature]>=root.split_pos:\n",
    "                prediction.append(1 if root.positive>root.negative else 0)\n",
    "                return\n",
    "\n",
    "            if root.left==None and df[root.feature]<root.split_pos:\n",
    "                prediction.append(1 if root.positive>root.negative else 0)\n",
    "                return\n",
    "\n",
    "            if df[root.feature]<root.split_pos:\n",
    "                rec_predict(df,root.left,prediction)\n",
    "            else:\n",
    "                rec_predict(df,root.right,prediction)\n",
    "        except KeyError:\n",
    "            if root.left==None:\n",
    "                prediction.append(1 if root.positive>root.negative else 0)\n",
    "                return\n",
    "            rec_predict(df,root.left,prediction)\n",
    "    else:\n",
    "        try:\n",
    "            if root.right==None and root.left==None:\n",
    "                prediction.append(1 if root.positive>root.negative else 0)\n",
    "                return\n",
    "\n",
    "            if root.right==None and df[root.feature]==1:\n",
    "                prediction.append(1 if root.positive>root.negative else 0)\n",
    "                return\n",
    "\n",
    "            if root.left==None and df[root.feature]==0:\n",
    "                prediction.append(1 if root.positive>root.negative else 0)\n",
    "                return\n",
    "\n",
    "            if df[root.feature]==0:\n",
    "                rec_predict(df,root.left,prediction)\n",
    "            else:\n",
    "                rec_predict(df,root.right,prediction)\n",
    "        except KeyError:\n",
    "            if root.left==None:\n",
    "                prediction.append(1 if root.positive>root.negative else 0)\n",
    "                return\n",
    "            rec_predict(df,root.left,prediction)\n",
    "            \n",
    "def predict(df,root,prediction):\n",
    "    for col,row in df.iterrows():\n",
    "        rec_predict(row,root,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### perform prediction with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1649   62]\n",
      " [  51  486]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      1711\n",
      "           1       0.89      0.91      0.90       537\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2248\n",
      "   macro avg       0.93      0.93      0.93      2248\n",
      "weighted avg       0.95      0.95      0.95      2248\n",
      "\n",
      "0.9497330960854092\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "X1_test = X_test.copy(deep=True)\n",
    "\n",
    "prediction = []\n",
    "# preprocess(X_test,split_values)\n",
    "predict(X_test,root,prediction)\n",
    "print confusion_matrix(Y_test,prediction)\n",
    "print classification_report(Y_test,prediction)\n",
    "print accuracy_score(Y_test,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### perform prediciton with inbuilt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1660   51]\n",
      " [  23  514]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1711\n",
      "           1       0.91      0.96      0.93       537\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2248\n",
      "   macro avg       0.95      0.96      0.96      2248\n",
      "weighted avg       0.97      0.97      0.97      2248\n",
      "\n",
      "0.9670818505338078\n"
     ]
    }
   ],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "Y_predict = model.predict(X1_test)\n",
    "print confusion_matrix(Y_test,Y_predict)\n",
    "print classification_report(Y_test,Y_predict)\n",
    "print accuracy_score(Y_test,Y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANS  2\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "ot = []\n",
    "test_rows=pd.read_csv('../input_data/sample_test.csv')\n",
    "test_rows1 = pd.concat([test_rows,pd.get_dummies(test_rows['sales'],prefix='sales')],axis=1)\n",
    "test_rows1 = pd.concat([test_rows1,pd.get_dummies(test_rows1['salary'],prefix='salary')],axis=1)\n",
    "test_rows1 = test_rows1.drop(['sales','salary'],axis=1)\n",
    "# preprocess(test_rows1,split_values)\n",
    "predict(test_rows1,root,ot)\n",
    "print \"ANS \", len(ot)\n",
    "print ot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "Using both numerical and categorical feature improved result over using only categorical feature\n",
    "\n",
    "To handle numerical features, best split point is to be found on every node of the tree\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
