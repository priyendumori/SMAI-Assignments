{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q-1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
      "0           242        317          103                  2  2.5   2.0  8.15   \n",
      "1           334        319          108                  3  3.0   3.5  8.54   \n",
      "2             4        322          110                  3  3.5   2.5  8.67   \n",
      "3            45        326          113                  5  4.5   4.0  9.40   \n",
      "4           232        319          106                  3  3.5   2.5  8.33   \n",
      "5           167        302          102                  3  3.5   5.0  8.33   \n",
      "6           166        322          110                  5  4.5   4.0  8.97   \n",
      "7           196        307          107                  2  3.0   3.5  8.52   \n",
      "8           147        315          105                  3  2.0   2.5  8.48   \n",
      "9           389        296           97                  2  1.5   2.0  7.80   \n",
      "10          282        317          110                  3  4.0   4.5  9.11   \n",
      "11          426        323          111                  5  4.0   5.0  9.86   \n",
      "12          478        309          105                  4  3.5   2.0  8.18   \n",
      "13          374        321          109                  3  3.0   3.0  8.54   \n",
      "14          214        333          119                  5  5.0   4.5  9.78   \n",
      "15           90        316          109                  4  4.5   3.5  8.76   \n",
      "16          142        332          118                  2  4.5   3.5  9.36   \n",
      "17          258        324          100                  3  4.0   5.0  8.64   \n",
      "18          406        302           99                  3  2.5   3.0  7.45   \n",
      "19          457        299          100                  2  2.0   2.0  7.88   \n",
      "20          317        298          101                  2  1.5   2.0  7.86   \n",
      "21          294        312           98                  1  3.5   3.0  8.18   \n",
      "22          162        298           99                  1  1.5   3.0  7.46   \n",
      "23            8        308          101                  2  3.0   4.0  7.90   \n",
      "24          500        327          113                  4  4.5   4.5  9.04   \n",
      "25          274        312           99                  1  1.0   1.5  8.01   \n",
      "26           11        325          106                  3  3.5   4.0  8.40   \n",
      "27          455        310          105                  2  3.0   3.5  8.01   \n",
      "28           35        331          112                  5  4.0   5.0  9.80   \n",
      "29          143        331          115                  5  4.0   3.5  9.44   \n",
      "..          ...        ...          ...                ...  ...   ...   ...   \n",
      "420         400        333          117                  4  5.0   4.0  9.66   \n",
      "421         454        319          103                  3  2.5   4.0  8.76   \n",
      "422          66        325          112                  4  3.5   3.5  8.92   \n",
      "423         137        312          103                  3  5.0   4.0  8.45   \n",
      "424          13        328          112                  4  4.0   4.5  9.10   \n",
      "425         259        326          102                  4  5.0   5.0  8.76   \n",
      "426          69        318          109                  3  3.5   4.0  9.22   \n",
      "427         346        316           98                  1  1.5   2.0  7.43   \n",
      "428         191        324          111                  5  4.5   4.0  9.16   \n",
      "429          20        303          102                  3  3.5   3.0  8.50   \n",
      "430           7        321          109                  3  3.0   4.0  8.20   \n",
      "431         199        311          104                  3  4.5   4.5  8.43   \n",
      "432         323        314          107                  2  2.5   4.0  8.27   \n",
      "433         239        310          104                  3  2.0   3.5  8.37   \n",
      "434         475        308          105                  4  3.0   2.5  7.95   \n",
      "435         138        316          100                  2  1.5   3.0  8.16   \n",
      "436         424        334          119                  5  4.5   5.0  9.54   \n",
      "437         114        320          110                  2  4.0   3.5  8.56   \n",
      "438          47        329          114                  5  4.0   5.0  9.30   \n",
      "439         198        310          106                  2  3.5   2.5  8.33   \n",
      "440         377        297           96                  2  2.5   2.0  7.43   \n",
      "441          70        328          115                  4  4.5   4.0  9.16   \n",
      "442         402        315          105                  2  3.0   3.0  8.34   \n",
      "443          30        310           99                  2  1.5   2.0  7.30   \n",
      "444         366        330          114                  4  4.5   3.0  9.17   \n",
      "445          31        300           97                  2  3.0   3.0  8.10   \n",
      "446         427        312          106                  3  3.0   5.0  8.57   \n",
      "447          67        327          114                  3  3.0   3.0  9.02   \n",
      "448           1        337          118                  4  4.5   4.5  9.65   \n",
      "449         399        312          103                  3  3.5   4.0  8.78   \n",
      "\n",
      "     Research  Chance of Admit   \n",
      "0           0              0.65  \n",
      "1           1              0.71  \n",
      "2           1              0.80  \n",
      "3           1              0.91  \n",
      "4           1              0.74  \n",
      "5           0              0.65  \n",
      "6           0              0.78  \n",
      "7           1              0.78  \n",
      "8           0              0.75  \n",
      "9           0              0.49  \n",
      "10          1              0.80  \n",
      "11          1              0.92  \n",
      "12          0              0.65  \n",
      "13          1              0.79  \n",
      "14          1              0.96  \n",
      "15          1              0.74  \n",
      "16          1              0.90  \n",
      "17          1              0.78  \n",
      "18          0              0.52  \n",
      "19          0              0.51  \n",
      "20          0              0.54  \n",
      "21          1              0.64  \n",
      "22          0              0.53  \n",
      "23          0              0.68  \n",
      "24          0              0.84  \n",
      "25          1              0.52  \n",
      "26          1              0.52  \n",
      "27          0              0.71  \n",
      "28          1              0.94  \n",
      "29          1              0.92  \n",
      "..        ...               ...  \n",
      "420         1              0.95  \n",
      "421         1              0.73  \n",
      "422         0              0.55  \n",
      "423         0              0.76  \n",
      "424         1              0.78  \n",
      "425         1              0.77  \n",
      "426         1              0.68  \n",
      "427         0              0.49  \n",
      "428         1              0.90  \n",
      "429         0              0.62  \n",
      "430         1              0.75  \n",
      "431         0              0.70  \n",
      "432         0              0.72  \n",
      "433         0              0.70  \n",
      "434         1              0.67  \n",
      "435         1              0.71  \n",
      "436         1              0.94  \n",
      "437         0              0.72  \n",
      "438         1              0.86  \n",
      "439         0              0.73  \n",
      "440         0              0.34  \n",
      "441         1              0.78  \n",
      "442         0              0.66  \n",
      "443         0              0.54  \n",
      "444         1              0.86  \n",
      "445         1              0.65  \n",
      "446         0              0.71  \n",
      "447         0              0.61  \n",
      "448         1              0.92  \n",
      "449         0              0.67  \n",
      "\n",
      "[450 rows x 9 columns]\n",
      "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
      "0           242        317          103                  2  2.5   2.0  8.15   \n",
      "1           334        319          108                  3  3.0   3.5  8.54   \n",
      "2             4        322          110                  3  3.5   2.5  8.67   \n",
      "3            45        326          113                  5  4.5   4.0  9.40   \n",
      "4           232        319          106                  3  3.5   2.5  8.33   \n",
      "5           167        302          102                  3  3.5   5.0  8.33   \n",
      "6           166        322          110                  5  4.5   4.0  8.97   \n",
      "7           196        307          107                  2  3.0   3.5  8.52   \n",
      "8           147        315          105                  3  2.0   2.5  8.48   \n",
      "9           389        296           97                  2  1.5   2.0  7.80   \n",
      "10          282        317          110                  3  4.0   4.5  9.11   \n",
      "11          426        323          111                  5  4.0   5.0  9.86   \n",
      "12          478        309          105                  4  3.5   2.0  8.18   \n",
      "13          374        321          109                  3  3.0   3.0  8.54   \n",
      "14          214        333          119                  5  5.0   4.5  9.78   \n",
      "15           90        316          109                  4  4.5   3.5  8.76   \n",
      "16          142        332          118                  2  4.5   3.5  9.36   \n",
      "17          258        324          100                  3  4.0   5.0  8.64   \n",
      "18          406        302           99                  3  2.5   3.0  7.45   \n",
      "19          457        299          100                  2  2.0   2.0  7.88   \n",
      "20          317        298          101                  2  1.5   2.0  7.86   \n",
      "21          294        312           98                  1  3.5   3.0  8.18   \n",
      "22          162        298           99                  1  1.5   3.0  7.46   \n",
      "23            8        308          101                  2  3.0   4.0  7.90   \n",
      "24          500        327          113                  4  4.5   4.5  9.04   \n",
      "25          274        312           99                  1  1.0   1.5  8.01   \n",
      "26           11        325          106                  3  3.5   4.0  8.40   \n",
      "27          455        310          105                  2  3.0   3.5  8.01   \n",
      "28           35        331          112                  5  4.0   5.0  9.80   \n",
      "29          143        331          115                  5  4.0   3.5  9.44   \n",
      "..          ...        ...          ...                ...  ...   ...   ...   \n",
      "420         400        333          117                  4  5.0   4.0  9.66   \n",
      "421         454        319          103                  3  2.5   4.0  8.76   \n",
      "422          66        325          112                  4  3.5   3.5  8.92   \n",
      "423         137        312          103                  3  5.0   4.0  8.45   \n",
      "424          13        328          112                  4  4.0   4.5  9.10   \n",
      "425         259        326          102                  4  5.0   5.0  8.76   \n",
      "426          69        318          109                  3  3.5   4.0  9.22   \n",
      "427         346        316           98                  1  1.5   2.0  7.43   \n",
      "428         191        324          111                  5  4.5   4.0  9.16   \n",
      "429          20        303          102                  3  3.5   3.0  8.50   \n",
      "430           7        321          109                  3  3.0   4.0  8.20   \n",
      "431         199        311          104                  3  4.5   4.5  8.43   \n",
      "432         323        314          107                  2  2.5   4.0  8.27   \n",
      "433         239        310          104                  3  2.0   3.5  8.37   \n",
      "434         475        308          105                  4  3.0   2.5  7.95   \n",
      "435         138        316          100                  2  1.5   3.0  8.16   \n",
      "436         424        334          119                  5  4.5   5.0  9.54   \n",
      "437         114        320          110                  2  4.0   3.5  8.56   \n",
      "438          47        329          114                  5  4.0   5.0  9.30   \n",
      "439         198        310          106                  2  3.5   2.5  8.33   \n",
      "440         377        297           96                  2  2.5   2.0  7.43   \n",
      "441          70        328          115                  4  4.5   4.0  9.16   \n",
      "442         402        315          105                  2  3.0   3.0  8.34   \n",
      "443          30        310           99                  2  1.5   2.0  7.30   \n",
      "444         366        330          114                  4  4.5   3.0  9.17   \n",
      "445          31        300           97                  2  3.0   3.0  8.10   \n",
      "446         427        312          106                  3  3.0   5.0  8.57   \n",
      "447          67        327          114                  3  3.0   3.0  9.02   \n",
      "448           1        337          118                  4  4.5   4.5  9.65   \n",
      "449         399        312          103                  3  3.5   4.0  8.78   \n",
      "\n",
      "     Research  Chance of Admit   \n",
      "0           0               1.0  \n",
      "1           1               1.0  \n",
      "2           1               1.0  \n",
      "3           1               1.0  \n",
      "4           1               1.0  \n",
      "5           0               1.0  \n",
      "6           0               1.0  \n",
      "7           1               1.0  \n",
      "8           0               1.0  \n",
      "9           0               0.0  \n",
      "10          1               1.0  \n",
      "11          1               1.0  \n",
      "12          0               1.0  \n",
      "13          1               1.0  \n",
      "14          1               1.0  \n",
      "15          1               1.0  \n",
      "16          1               1.0  \n",
      "17          1               1.0  \n",
      "18          0               1.0  \n",
      "19          0               1.0  \n",
      "20          0               1.0  \n",
      "21          1               1.0  \n",
      "22          0               1.0  \n",
      "23          0               1.0  \n",
      "24          0               1.0  \n",
      "25          1               1.0  \n",
      "26          1               1.0  \n",
      "27          0               1.0  \n",
      "28          1               1.0  \n",
      "29          1               1.0  \n",
      "..        ...               ...  \n",
      "420         1               1.0  \n",
      "421         1               1.0  \n",
      "422         0               1.0  \n",
      "423         0               1.0  \n",
      "424         1               1.0  \n",
      "425         1               1.0  \n",
      "426         1               1.0  \n",
      "427         0               0.0  \n",
      "428         1               1.0  \n",
      "429         0               1.0  \n",
      "430         1               1.0  \n",
      "431         0               1.0  \n",
      "432         0               1.0  \n",
      "433         0               1.0  \n",
      "434         1               1.0  \n",
      "435         1               1.0  \n",
      "436         1               1.0  \n",
      "437         0               1.0  \n",
      "438         1               1.0  \n",
      "439         0               1.0  \n",
      "440         0               0.0  \n",
      "441         1               1.0  \n",
      "442         0               1.0  \n",
      "443         0               1.0  \n",
      "444         1               1.0  \n",
      "445         1               1.0  \n",
      "446         0               1.0  \n",
      "447         0               1.0  \n",
      "448         1               1.0  \n",
      "449         0               1.0  \n",
      "\n",
      "[450 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyendu/.local/lib/python2.7/site-packages/ipykernel_launcher.py:11: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"input_data/AdmissionDataset/data.csv\")\n",
    "print df\n",
    "df.loc[df['Chance of Admit ']<0.5,'Chance of Admit '] = 0\n",
    "df.loc[df['Chance of Admit ']>=0.5,'Chance of Admit '] = 1\n",
    "print df\n",
    "X = df.drop(['Serial No.','Chance of Admit '],axis=1)\n",
    "Y = df['Chance of Admit ']\n",
    "\n",
    "col_names = [i for i in X]\n",
    "X = pd.DataFrame(preprocessing.scale(X), columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "regressor=LinearRegression()\n",
    "regressor.fit(X_train,Y_train)\n",
    "pred = regressor.predict(X_test) \n",
    "print(regressor.coef_)\n",
    "print(regressor.intercept_)\n",
    "r2_score(Y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0]\n",
      " [ 0 79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        11\n",
      "         1.0       1.00      1.00      1.00        79\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        90\n",
      "   macro avg       1.00      1.00      1.00        90\n",
      "weighted avg       1.00      1.00      1.00        90\n",
      "\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(X_train, Y_train)\n",
    "Z = logreg.predict(X_test)\n",
    "print confusion_matrix(Y_test,Z)\n",
    "print classification_report(Y_test,Z)\n",
    "print accuracy_score(Y_test,Z)\n",
    "\n",
    "score = logreg.score(X_test,Y_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train.reset_index(drop=True)\n",
    "Y_train1 = Y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = pd.DataFrame(1,index=np.arange(X_train.shape[0]),columns=[\"ones\"])\n",
    "X_train1 = pd.concat([ones, X_train1],axis=1)\n",
    "X_train1 = np.array(X_train1)\n",
    "Y_train1 = np.array(Y_train1).reshape(X_train1.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros([1,8])\n",
    "alpha = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X):\n",
    "    X=-X\n",
    "    return 1/(1+np.exp(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,Y,theta,it,alpha):\n",
    "    for i in range(it):\n",
    "        theta = theta - (alpha) * np.sum(X * (h(np.matmul(X, theta.T)) - Y), axis=0)\n",
    "    return theta\n",
    "\n",
    "g = gradientDescent(X_train1,Y_train1,theta,iterations,alpha)\n",
    "theta_list = g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test):\n",
    "    Y_pred=[]\n",
    "    for index,row in X_test.iterrows():\n",
    "        row=list(row)\n",
    "        y1=0\n",
    "        for i in range(1,8):\n",
    "            y1=y1+theta_list[i]*row[i-1]\n",
    "        y1=y1+theta_list[0]\n",
    "        Y_pred.append(0 if y1<0.5 else 1)\n",
    "    return Y_pred\n",
    "pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0]\n",
      " [ 0 79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        11\n",
      "         1.0       1.00      1.00      1.00        79\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        90\n",
      "   macro avg       1.00      1.00      1.00        90\n",
      "weighted avg       1.00      1.00      1.00        90\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# print r2_score(list(Y_test),pred)\n",
    "# print theta_list\n",
    "\n",
    "print confusion_matrix(Y_test,pred)\n",
    "print classification_report(Y_test,pred)\n",
    "print accuracy_score(Y_test,pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
