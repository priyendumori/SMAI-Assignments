{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q-1-3-2\n",
    "#### Compare  the  performance  of  Mean  square  error  loss  function  vs  Mean  Absolute error function vs Mean absolute percentage error function and explain the reasons for the observed behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priyendu/.local/lib/python2.7/site-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"input_data/AdmissionDataset/data.csv\")\n",
    "X = df.drop(['Serial No.','Chance of Admit '],axis=1)\n",
    "Y = df['Chance of Admit ']\n",
    "col_names = [i for i in X]\n",
    "X = pd.DataFrame(preprocessing.scale(X), columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8057053567883976"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor=LinearRegression()\n",
    "regressor.fit(X_train,Y_train)\n",
    "sys_pred = regressor.predict(X_test) \n",
    "inbuilt_coeff = []\n",
    "inbuilt_coeff.append(regressor.intercept_)\n",
    "inbuilt_coeff.append(list(regressor.coef_))\n",
    "# print(regressor.coef_)\n",
    "# print(regressor.intercept_)\n",
    "r2_score(Y_test,sys_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train.reset_index(drop=True)\n",
    "Y_train1 = Y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = pd.DataFrame(1,index=np.arange(X_train.shape[0]),columns=[\"ones\"])\n",
    "X_train1 = pd.concat([ones, X_train1],axis=1)\n",
    "X_train1 = np.array(X_train1)\n",
    "Y_train1 = np.array(Y_train1).reshape(X_train1.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros([1,8])\n",
    "alpha = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,Y,theta,it,alpha):\n",
    "    for i in range(it):\n",
    "        theta = theta - (alpha/len(X)) * np.sum(X * (np.matmul(X, theta.T) - Y), axis=0)\n",
    "    return theta\n",
    "\n",
    "g = gradientDescent(X_train1,Y_train1,theta,iterations,alpha)\n",
    "theta_list = g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test):\n",
    "    Y_pred=[]\n",
    "    for index,row in X_test.iterrows():\n",
    "        row=list(row)\n",
    "        y1=0\n",
    "        for i in range(1,8):\n",
    "            y1=y1+theta_list[i]*row[i-1]\n",
    "        y1=y1+theta_list[0]\n",
    "        Y_pred.append(y1)\n",
    "    return Y_pred\n",
    "pred = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8102925587234204\n",
      "[0.72134463 0.02671717 0.01566358 0.00871445 0.00521958 0.01430209\n",
      " 0.06208434 0.01353053]\n"
     ]
    }
   ],
   "source": [
    "print r2_score(list(Y_test),pred)\n",
    "print theta_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### methods to calculate all types of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_percentage_error(Y_test,pred):\n",
    "    return np.mean(np.abs( Y_test - pred ) / Y_test) * 100.0\n",
    "\n",
    "def mean_asolute_error(Y_test,pred):\n",
    "    return np.mean(np.abs( Y_test - pred ))\n",
    "\n",
    "def mean_squared_error(Y_test,pred):\n",
    "    return np.mean( ( Y_test - pred )**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My model\n",
      "7.152522566023381\n",
      "0.044582488684379376\n",
      "0.003976680172980129\n",
      "\n",
      "System's model\n",
      "7.170176881890979\n",
      "0.04471720166265667\n",
      "0.004072837892791795\n"
     ]
    }
   ],
   "source": [
    "print \"My model\"\n",
    "print mean_percentage_error(Y_test,pred)\n",
    "print mean_asolute_error(Y_test,pred)\n",
    "print mean_squared_error(Y_test,pred)\n",
    "\n",
    "print \"\\nSystem's model\"\n",
    "print mean_percentage_error(Y_test,sys_pred)\n",
    "print mean_asolute_error(Y_test,sys_pred)\n",
    "print mean_squared_error(Y_test,sys_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Observation\n",
    "* MAE gives less weight to outliers, which is not sensitive to outliers.\n",
    "\n",
    "* MSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 10 is more than twice as bad as being off by 5. But if being off by 10 is just twice as bad as being off by 5, then MAE is more appropriate.\n",
    "\n",
    "* Similar to MAE, but normalized by true observation. Downside is when true obs is zero, this metric will be problematic.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
